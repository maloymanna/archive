<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Spark on Maloy Manna</title>
    <link>https://maloymanna.github.io/tags/spark/</link>
    <description>Recent content in Spark on Maloy Manna</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en-us</language>
    <copyright>&amp;copy; 2007 - 2020. All rights reserved.</copyright>
    <lastBuildDate>Wed, 18 Nov 2015 11:37:49 +0000</lastBuildDate>
    
	<atom:link href="https://maloymanna.github.io/tags/spark/index.xml" rel="self" type="application/rss+xml" />
    
    
    <item>
      <title>Data processing with Spark in R &amp; Python</title>
      <link>https://maloymanna.github.io/2015/11/18/data-processing-with-spark-in-r-python/</link>
      <pubDate>Wed, 18 Nov 2015 11:37:49 +0000</pubDate>
      
      <guid>https://maloymanna.github.io/2015/11/18/data-processing-with-spark-in-r-python/</guid>
      <description>I recently gave a talk on data processing with Apache Spark using R and Python. tl;dr - the slides and presentation can be accessed below (free registration):
   As noted in my previous post, Spark has become the defacto standard for big data applications and has been adopted quickly by the industry. See Cloudera&amp;rsquo;s One Platform initiative blog post by CEO Mike Olson for their commitment to Spark.</description>
    </item>
    
    <item>
      <title>Set up a Hadoop Spark cluster in 10 minutes with Vagrant</title>
      <link>https://maloymanna.github.io/2014/12/30/set-up-a-hadoop-spark-cluster-in-10-minutes-with-vagrant/</link>
      <pubDate>Tue, 30 Dec 2014 22:36:53 +0000</pubDate>
      
      <guid>https://maloymanna.github.io/2014/12/30/set-up-a-hadoop-spark-cluster-in-10-minutes-with-vagrant/</guid>
      <description>With each of the big 3 Hadoop vendors - Cloudera, Hortonworks and MapR each providing their own Hadoop sandbox virtual machines (VMs), trying out Hadoop today has become extremely easy. For a developer, it is extremely useful to download a get started with one of these VMs and try out Hadoop to practice data science right away.
However, with the core Apache Hadoop, these vendors package their own software into their distributions, mostly for the orchestration and management, which can be a pain due to the multiple scattered open-source projects within the Hadoop ecosystem.</description>
    </item>
    
    <item>
      <title>Basics of Big Data – Part 2 - Hadoop</title>
      <link>https://maloymanna.github.io/2014/04/13/basics-of-big-data-part-2-hadoop/</link>
      <pubDate>Sun, 13 Apr 2014 18:12:16 +0000</pubDate>
      
      <guid>https://maloymanna.github.io/2014/04/13/basics-of-big-data-part-2-hadoop/</guid>
      <description>As discussed in Part 1 of this series, Hadoop is the foremost among tools being currently used for deriving value out of Big Data. The process of gaining insights from data through Business Intelligence and analytics essentially remains the same. However, with the huge variety, volume and velocity (the 3Vs of Big Data), it’s become necessary to re-think of the data management infrastructure. Hadoop, originally designed to be used with the MapReduce algorithm to solve parallel processing constraints in distributed architectures (e.</description>
    </item>
    
  </channel>
</rss>